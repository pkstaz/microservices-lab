== Change Data Capture with Kafka Connect and Debezium
:imagesdir: ./images

This part of the lab introduces you to change data capture (CDC) using http://debezium.io/[Debezium].
Debezium allows you to capture data changes from MySQL, PostgreSQL, MongoDB, Oracle and SQL Server and stream those events into Apache Kafka.
It is based on https://kafka.apache.org/documentation/#connect[Kafka Connect].
Streaming data changes from your database enables many interesting use cases such as

* Replicating data across databases by different vendors
* Synchronizing data between microservices
* Updating fulltext search indexes (e.g. Elasticsearch) or caches (e.g. https://www.redhat.com/en/technologies/jboss-middleware/data-grid[JBoss Data Grid])
* Running stream queries such as sliding averages of the value of incoming orders etc.

In this module of the lab you'll learn the following things:

* Setting up Kafka Connect with the Debezium CDC connector for MySQL
* Configuring an instance of that connector, ingesting changes from an existing example CRUD application
* Different ways of processing change data events
** Streaming data changes to a PostgreSQL sink database
** Streaming data changes to an Elasticsearch index

The overall architecture of this lab module looks like this:

image::debezium-demo.png[Module 3 Architecture Overview]

=== Setting Up the Example Application

We've prepared a small https://github.com/debezium/microservices-lab/tree/master/eventr[CRUD microservice for managing orders of events], which we'll use in the following as source of data change events.
It is based on https://thorntail.io/[Thorntail], an approach for building microservices using Java EE and MicroProfile, and uses MySQL as its database.

Start a MySQL instance by running:

[source, sh]
$ oc new-app --name=mysql debezium/example-mysql:0.9 \
    -e MYSQL_ROOT_PASSWORD=debezium \
    -e MYSQL_USER=mysqluser \
    -e MYSQL_PASSWORD=mysqlpw

On your host machine, check out a new branch for this module of the workshop and push it to your fork on GitHub:

[source, sh]
----
# In microservices-lab
git checkout -b module3 upstream/module3-start
git push origin module3
----

Then, in the VM running OpenShift, build and start the application.
Again we're using the source-to-image process for that:

[source,sh]
----
$ oc new-app --name=eventrapp debezium/msa-lab-s2i:latest~https://github.com/debezium/microservices-lab \
    --context-dir=eventr \
    -e AB_PROMETHEUS_OFF=true \
    -e JAVA_OPTIONS=-Djava.net.preferIPv4Stack=true
----

We still need to expose port 8080 for the application and set up a route for it
(as that's not done automatically by the S2I builder image).
To do so, use `oc patch` and expose a route for the service like so:

[source,sh]
----
$ oc patch service eventrapp -p '{ "spec" : { "ports" : [{ "name" : "8080-tcp", "port" : 8080, "protocol" : "TCP", "targetPort" : 8080 }] } } }'

$ oc expose svc eventrapp
----

Verify that the pods of the database and the application are running:

[source,sh]
----
$ oc get pods
----

Once the build has completed, you should see the following status
(the build job must be "Completed", the actual application and the database "Running"):

[source,sh]
----
NAME                                          READY     STATUS    RESTARTS   AGE
eventrapp-1-build                             0/1       Completed   0          2m
eventrapp-1-m4rss                             1/1       Running     0          39s
mysql-2-gk8nw                                 1/1       Running     0          2m
...
----

The hostname of the exposed eventrapp service is available in the OpenShift console, or can be retrieved via CLI:

[source]
$ oc get routes eventrapp -o=jsonpath='{.spec.host}{"\n"}'

Note the output, which should be in the format eventrapp-voxxed.192.168.33.10.nip.io and you can http://eventrapp-voxxed.192.168.33.10.nip.io[navigate there] using a browser.
Alternatively, you could click on the application's URL within the OpenShift web UI.

You can use "Populate Data" to create some example data (Click on "Search orders" after that to see the created entries).
Otherwise just insert a few records as you like.

=== Setting Up Kafka Connect With the Debezium Connectors

Now it's time to deploy Kafka Connect and Debezium.
Kafka Connect is a platform for streaming data between Apache Kafka and other systems.
It provides a framework and runtime environment for _source connectors_ (for getting data into Kafka)
and _sink connectors_ (for getting data out of Kafka).
Debezium provides a set of CDC source connectors for databases such as MySQL, PostgreSQL and MongoDB.

Let's begin by setting up a Kafka Connect cluster,
using the S2I process provided by Strimzi.
This makes it very easy to create a Kafka Connect cluster with additional connectors such as the ones provided by Debezium (the operation must be executed as the `admin`):

[source]
----
$ oc apply -f examples/templates/cluster-operator/connect-s2i-template.yaml -n amq-streams

$ oc process strimzi-connect-s2i \
    -p CLUSTER_NAME=debezium \
    -p KAFKA_CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=1 \
    -p KAFKA_CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=1 \
    -p KAFKA_CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=1 \
    -p KAFKA_CONNECT_BOOTSTRAP_SERVERS=production-ready-kafka-bootstrap:9092 \
    | oc apply -f -
----

Use `oc status` to verify that the S2I process has finished
(a deployment of `svc/debezium-connect-api` should be in state "deployed"):

[source]
----
$ oc status

In project amq-streams on server https://master00.example.com:443

svc/debezium-connect-api - 172.30.17.34:8083
  dc/debezium-connect deploys istag/debezium-connect:latest <-
    bc/debezium-connect source builds uploaded code on istag/debezium-connect-source:1.0.0
      not built yet
    deployment #2 deployed about a minute ago - 1 pod
    deployment #1 failed 2 minutes ago: newer deployment was found running
...
----

Alternatively, you can check in the OpenShift web UI that the rolling deployment for "debezium-connect" has finished.

Then download the following files:

* the Debezium CDC connector for MySQL
* the Confluent JDBC sink connector and the PostgreSQL database driver
* the Confluent Elasticsearch sink connector and its dependencies

Extract the downloaded files and trigger another S2I build of `debezium-connect`, this time including these additional resources:

[source,sh]
----
export DEBEZIUM_VERSION=0.9.4.Final
mkdir -p plugins && cd plugins && \
curl http://central.maven.org/maven2/io/debezium/debezium-connector-mysql/$DEBEZIUM_VERSION/debezium-connector-mysql-$DEBEZIUM_VERSION-plugin.tar.gz | tar xz; \
curl http://central.maven.org/maven2/io/debezium/debezium-connector-postgres/$DEBEZIUM_VERSION/debezium-connector-postgres-$DEBEZIUM_VERSION-plugin.tar.gz | tar xz; \
mkdir confluent-jdbc-sink && cd confluent-jdbc-sink && \
curl -O http://central.maven.org/maven2/org/postgresql/postgresql/42.2.2/postgresql-42.2.2.jar && \
curl -O http://packages.confluent.io/maven/io/confluent/kafka-connect-jdbc/5.0.0/kafka-connect-jdbc-5.0.0.jar && \
cd .. && \
mkdir confluent-es-sink && cd confluent-es-sink && \
curl -sO http://packages.confluent.io/maven/io/confluent/kafka-connect-elasticsearch/5.0.0/kafka-connect-elasticsearch-5.0.0.jar && \
curl -sO http://central.maven.org/maven2/io/searchbox/jest/2.0.0/jest-2.0.0.jar && \
curl -sO http://central.maven.org/maven2/org/apache/httpcomponents/httpcore-nio/4.4.4/httpcore-nio-4.4.4.jar && \
curl -sO http://central.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.1/httpclient-4.5.1.jar && \
curl -sO http://central.maven.org/maven2/org/apache/httpcomponents/httpasyncclient/4.1.1/httpasyncclient-4.1.1.jar && \
curl -sO http://central.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar && \
curl -sO http://central.maven.org/maven2/commons-logging/commons-logging/1.2/commons-logging-1.2.jar && \
curl -sO http://central.maven.org/maven2/commons-codec/commons-codec/1.9/commons-codec-1.9.jar && \
curl -sO http://central.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar && \
curl -sO http://central.maven.org/maven2/io/searchbox/jest-common/2.0.0/jest-common-2.0.0.jar && \
curl -sO http://central.maven.org/maven2/com/google/code/gson/gson/2.4/gson-2.4.jar && \
cd .. && \
oc start-build debezium-connect --from-dir=. --follow && \
cd ..
----

You should see an output like this:

[source]
----
Uploading directory "." as binary input for the build ...
build "debezium-connect-2" started
Receiving source from STDIN as archive ...
Assembling plugins into custom plugin directory /tmp/kafka-plugins
Moving plugins to /tmp/kafka-plugins
Pushing image docker-registry.default.svc:5000/l1099-kafka/debezium-connect:latest ...
Pushed 6/9 layers, 67% complete
Pushed 7/9 layers, 78% complete
Pushed 8/9 layers, 89% complete
Pushed 9/9 layers, 100% complete
Push successful
----

Use `oc get pods` again to verify that Kafka Connect is running:

[source,sh]
----
$ oc get pods

NAME                                          READY     STATUS    RESTARTS   AGE
debezium-connect-3-mpscv                      1/1       Running     0          1m
...
----

Once that's the case, register an instance of the Debezium MySQL connector using the REST API of Kafka Connect:

[source]
----
$ oc exec -c kafka -i production-ready-kafka-0 -- curl -s  -w "\n" -X POST \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect-api:8083/connectors -d @- <<'EOF'

{
    "name": "inventory-connector",
    "config": {
        "connector.class": "io.debezium.connector.mysql.MySqlConnector",
        "tasks.max": "1",
        "database.hostname": "mysql",
        "database.port": "3306",
        "database.user": "debezium",
        "database.password": "dbz",
        "database.server.id": "184054",
        "database.server.name": "dbserver1",
        "database.whitelist": "inventory",
        "database.history.kafka.bootstrap.servers": "production-ready-kafka-bootstrap:9092",
        "database.history.kafka.topic": "schema-changes.inventory",
        "transforms": "route",
        "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
        "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
        "transforms.route.replacement": "$3"
    }
}
EOF
----

This sets up an instance of Debezium's `io.debezium.connector.mysql.MySqlConnector` class,
using the given credentials.
By specifying the `database.whitelist` option (or, on a more fine-grained level, `table.whitelist`), we can narrow down the set of captured tables.

Kafka Connectâ€™s log file should contain messages regarding execution of initial snapshot (look for log messages like "INFO Step 1 ..."):

[source,sh]
----
$ oc logs $(oc get pods -o name -l strimzi.io/name=debezium-connect)
----

You can examine CDC messages in Kafka using the console consumer (use Ctrl + C to exit the tool):

[source]
----
$ oc exec -c zookeeper -it production-ready-zookeeper-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
   --bootstrap-server production-ready-kafka-bootstrap:9092 \
   --from-beginning \
   --property print.key=true \
   --topic EventrOrder
----

At this point you should see messages originating from the initial snapshot performed by the connector.

Note that by default topic names follow the pattern "<db server name>.<db name>.<table name>".
By means of the `RegexRouter` in the connector configuration we've changed that so that the topic name is just the unqualified table name.
You should see messages comprising of a key and a value like the following (formatted for the sake readability),
representing the `Order` records as per the initial snapshot.

Key:

[source]
----
{
    "schema": {
        "type": "struct",
        "fields": [
            {
                "type": "int32",
                "optional": false,
                "field": "id"
            }
        ],
        "optional": false,
        "name": "dbserver1.inventory.EventrOrder.Key"
    },
    "payload": {
        "id": 4
    }
}
----

Value:

[source]
----
{
    "schema": {
        "type": "struct",
        "fields": [
            {
                "type": "struct",
                "fields": [
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "id"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "customer"
                    },
                    {
                        "type": "int32",
                        "optional": false,
                        "name": "io.debezium.time.Date",
                        "version": 1,
                        "field": "order_date"
                    },
                    {
                        "type": "bytes",
                        "optional": false,
                        "name": "org.apache.kafka.connect.data.Decimal",
                        "version": 1,
                        "parameters": {
                            "scale": "2",
                            "connect.decimal.precision": "19"
                        },
                        "field": "payment"
                    },
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "event_id"
                    }
                ],
                "optional": true,
                "name": "dbserver1.inventory.EventrOrder.Value",
                "field": "before"
            },
            {
                "type": "struct",
                "fields": [
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "id"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "customer"
                    },
                    {
                        "type": "int32",
                        "optional": false,
                        "name": "io.debezium.time.Date",
                        "version": 1,
                        "field": "order_date"
                    },
                    {
                        "type": "bytes",
                        "optional": false,
                        "name": "org.apache.kafka.connect.data.Decimal",
                        "version": 1,
                        "parameters": {
                            "scale": "2",
                            "connect.decimal.precision": "19"
                        },
                        "field": "payment"
                    },
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "event_id"
                    }
                ],
                "optional": true,
                "name": "dbserver1.inventory.EventrOrder.Value",
                "field": "after"
            },
            {
                "type": "struct",
                "fields": [
                    {
                        "type": "string",
                        "optional": true,
                        "field": "version"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "name"
                    },
                    {
                        "type": "int64",
                        "optional": false,
                        "field": "server_id"
                    },
                    {
                        "type": "int64",
                        "optional": false,
                        "field": "ts_sec"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "gtid"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "file"
                    },
                    {
                        "type": "int64",
                        "optional": false,
                        "field": "pos"
                    },
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "row"
                    },
                    {
                        "type": "boolean",
                        "optional": true,
                        "default": false,
                        "field": "snapshot"
                    },
                    {
                        "type": "int64",
                        "optional": true,
                        "field": "thread"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "db"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "table"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "query"
                    }
                ],
                "optional": false,
                "name": "io.debezium.connector.mysql.Source",
                "field": "source"
            },
            {
                "type": "string",
                "optional": false,
                "field": "op"
            },
            {
                "type": "int64",
                "optional": true,
                "field": "ts_ms"
            }
        ],
        "optional": false,
        "name": "dbserver1.inventory.EventrOrder.Envelope"
    },
    "payload": {
        "before": null,
        "after": {
            "id": 4,
            "customer": "Bob Smith",
            "order_date": 17829,
            "payment": "F28=",
            "event_id": 1
        },
        "source": {
            "version": "0.8.3.Final",
            "name": "dbserver1",
            "server_id": 223344,
            "ts_sec": 1540457930,
            "gtid": null,
            "file": "mysql-bin.000003",
            "pos": 101280,
            "row": 0,
            "snapshot": false,
            "thread": 182,
            "db": "inventory",
            "table": "EventrOrder",
            "query": null
        },
        "op": "c",
        "ts_ms": 1540457964571
    }
}
----

Message key and value use JSON (the binary Avro format could be used alternatively),
and both contain a payload as well as a schema describing the structure of the payload.

The key's payload resembles the primary key of the represented record.
The value's payload contains information of

* the old state of the changed row (`before`, which is null in the case of an insert or record created during snapshotting)
* the new state of the changed row (`after`)
* metadata such as the table and database name, a timestamp etc.

If you now use the web app to insert, update or delete records while keeping the console consumer running, you'll see how corresponding CDC messages arrive in the topic.

Using the Kafka Connect REST API, you also can query the list of connectors, query the status of a given connector, delete a connector and more:

[source]
----
# List all connectors
$ oc exec -c kafka -i production-ready-kafka-0 -- curl -w "\n" -s -X GET \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect-api:8083/connectors
----

[source]
----
# Get status of "inventory-connector"
$ oc exec -c kafka -i production-ready-kafka-0 -- curl -w "\n" -s -X GET \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect-api:8083/connectors/inventory-connector/status
----

[source]
----
# Restart "inventory-connector"
$ oc exec -c kafka -i production-ready-kafka-0 -- curl -w "\n" -s -X POST \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect-api:8083/connectors/inventory-connector/restart
----

[source]
----
# Delete "inventory-connector" (don't run it, as we'll still need the connector in the following)
$ oc exec -c kafka -i production-ready-kafka-0 -- curl -w "\n" -s -X DELETE \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect-api:8083/connectors/inventory-connector
----

=== Processing Change Data Events

Examining change events in the Kafka console is a good first step,
but eventually we'd like to consume the events in a more meaningful way.

In the following different ways for consuming events are explored.
You can choose the one you are most interested in or walk through all the alternatives,
as your preference.

==== Streaming Data Changes to a PostgreSQL Sink Database

To stream data changes into another database, no manual programming effort is needed.
Instead, the Confluent JDBC sink connector for Kafka Connect can be used to data into a target database.

So let's set up another database (PostgreSQL in this case) and stream the data changes there.

[source]
----
$ oc new-app \
    -e POSTGRESQL_USER=postgresuser \
    -e POSTGRESQL_PASSWORD=postgrespw \
    -e POSTGRESQL_DATABASE=inventory \
    centos/postgresql-95-centos7
----

Once the database has started (use `oc get pods` to verify that PostgreSQL is running), register an instance of the https://docs.confluent.io/current/connect/kafka-connect-jdbc/sink-connector/index.html[Kafka Connect JDBC sink connector].
This connector can be used to propagate Kafka messages to relational databases via JDBC:

[source]
----
$ oc exec -c kafka -i production-ready-kafka-0 -- curl  -w "\n" -s -X POST \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect-api:8083/connectors -d @- <<'EOF'
{
    "name": "jdbc-sink",
    "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
        "tasks.max": "1",
        "topics": "EventrOrder",
        "connection.url": "jdbc:postgresql://postgresql-95-centos7:5432/inventory?user=postgresuser&password=postgrespw",
        "transforms": "unwrap",
        "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
        "auto.create": "true",
        "insert.mode": "upsert",
        "pk.fields": "id",
        "pk.mode": "record_value"
    }
}
EOF
----

This sets up an an instance of `io.confluent.connect.jdbc.JdbcSinkConnector`,
listening to the `EventrOrder` topic and streaming all data changes to the given database connection.
As this sink connector just expects the effective state of changed rows
(i.e. the "after" part from the Debezium data change messages),
only this part is extracted using Debezium's `UnwrapFromEnvelope` SMT (single message transform).

With the sink connector being set up, we can take a look into the PostgreSQL database and see how the table changes are propgated there.
Get a shell on the pod of the PostgreSQL service:

[source,sh]
----
$ oc rsh $(oc get pods -o name -l app=postgresql-95-centos7)
----

Run a query to get all records from the table corresponding to the monitored topic:

[source,sh]
----
psql -U postgresuser inventory -c 'select * from "EventrOrder"'
----

As you alter records in the source web application,
you'll see how the table in PostgreSQL gets updated accordingly, if you re-execute the query.
Note that `DELETE` operations currently cannot be propagated, as they are not yet supported by the JDBC sink connector.
Debezium provides a solution for that by allowing deletes to be rewritten into updates of a logical "deleted" field in emitted messages.
This can then be used to delete all records in the sink database e.g. using a batch job.

To leave the shell on the PostgreSQL pod, run:

[source]
----
exit
----

==== Streaming Change Events To Elasticsearch

The beauty of using Apache Kafka for streaming change events is its flexibility.
As the topics are persistent, additional consumers can come up which have not been known when data changes originally occurred.

As an example, lets stream the `EvntrOrder` events to Elasticsearch now, too, making them available to the powerful fulltext search capabilities.

Set up a single Elasticsearch node
(it'd be a complete cluster in production, but a single node is fine for the purposes of this lab)
and expose it as a service:

[source]
----
$ oc new-app -e ES_JAVA_OPTS="-Xms512m -Xmx512m" elasticsearch:6.4.2
$ oc expose svc/elasticsearch
----

Create a configuration file for Elasticsearch:

[source]
----
$ cat > elasticsearch.yml << EOF
cluster.name: docker-cluster123
network.host: 0.0.0.0
discovery.zen.minimum_master_nodes: 1
discovery.type: single-node
EOF
----

And make its contents available as a config map:

[source]
----
$ oc create configmap es-config --from-file=elasticsearch.yml
----

Finally, the config map contents can be exposed to the Elasticsearch container using a volume:

[source]
----
$ oc set volumes dc/elasticsearch --overwrite --add \
  -t configmap \
  -m /usr/share/elasticsearch/config/elasticsearch.yml \
  --sub-path=elasticsearch.yml \
  --name=es-config \
  --configmap-name=es-config
----

This triggers a restart of the Elasticsearch node; once it's up again (use `oc get pods` to verify),
it's time to register an instance of the Elasticsearch sink connector:

[source]
----
$ oc exec -c kafka -i production-ready-kafka-0 -- curl -X POST -s -w "\n" \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect-api:8083/connectors -d @- <<'EOF'
{
    "name": "elastic-sink",
    "config": {
        "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
        "tasks.max": "1",
        "topics": "EventrOrder",
        "connection.url": "http://elasticsearch:9200",
        "key.ignore": "false",
        "type.name": "order",
        "behavior.on.null.values" : "delete",
        "topic.index.map" : "EventrOrder:eventrorder",
        "transforms": "unwrap,key",
        "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
        "transforms.key.type": "org.apache.kafka.connect.transforms.ExtractField$Key",
        "transforms.key.field": "id"
    }
}
EOF
----

This listens to the `EventrOrder` topic and pushes corresponding index updates to Elasticsearch.
As index names must be lower-cased, the topic is named to the "eventrorder" index name.
The `UnwrapFromEnvelope` transformation is used to extract only the "after" state from Debezium's change events.
Using the `ExtractField` transformation we make sure that the original record id is used as the document id in Elasticsearch.

If the connector is deployed, you can query the index via its REST API.

Get its URL by executing:

[source]
$ oc get routes elasticsearch -o=jsonpath='{.spec.host}{"\n"}'

Open that URL in a browser, it should be in the form http://elasticsearch-voxxed.192.168.33.10.nip.io/.

To browse the contents of the `eventrorder` index, go to http://elasticsearch-voxxed.192.168.33.10.nip.io/eventrorder/_search?pretty.
Alternatively, you can use curl to do so:

[source]
$ oc exec -i production-ready-kafka-0 -- curl -s -X GET "elasticsearch:9200/eventrorder/_search?pretty"

Again create or update a few orders in the event application and you'll see how the Elasticsearch index is updated based on that.

To wrap up this section and make room for more explorations,
remove all the applications created during this part of the lab:

[source]
$ oc delete all -l app=eventrapp
$ oc delete all -l app=elasticsearch
$ oc delete all -l app=postgresql-95-centos7
$ oc delete all -l app=mysql

==== Explicit Event Structure with the Outbox Pattern

In this section we'll have a look at leveraging change data capture for the purposes of data exchange between microservices.
Typically, microservices don't exist in isolation but require data managed by other services in order to perform their tasks.
One common example are two microservices for processing purchase orders and handling shipments for such orders:
the shipment service will require the data on purchase orders managed by the order service.

There are different ways how the order service can notify the shipment service about new or updated orders;
for instance it could invoke a REST API provided by the shipment service.
This raises issues in terms of availability, though; if the shipment service cannot be reached,
the order service either cannot proceed at all or it will have to implement some kind of buffering of the API requests,
waiting until the shipment service becomes available again.

These issues are avoided by using asynchronous communication.
If the order service sends messages via a broker such as Apache Kafka,
it is not impacted by any downtimes of the shipment service.
As the order service will need to persist any incoming orders in its own local database, though,
it cannot directly send messages to Apache Kafka, as that would get us back to the "dual write" issues discussed in the previous module.
Also the service would now be tied to the availability of Kafka itself.

So what could an alternative be?
Change data capture shows a way out.
By streaming changes out of the order service's database,
all these issues are circumvented: the only resource that the order service relies on synchronously,
is its own database.
Now streaming changes of the services business tables (e.g. the table containing purchase orders)
might raise concerns about exposing implementation details to downstream consumers.
If for instance a column type changes, any consuming services will have to adjust accordingly in order to process the corresponding change events.
This is where the outbox pattern comes in: the idea is that the order service updates its actual busines tables
and at the same time *also* inserts an event record into a special "outbox" table within the same database.
These two actions happen within one transaction, so consistency is ensured.
Then change data capture and Debezium are used to capture only the changes (new entries) in this outbox table.
Via Apache Kafka, these events will be propagated to any consumers asynchronously, in a eventually consistent fashion.
The structure of the events (e.g. a JSON payload) is part of the order service's public API,
so great take care should be taken when altering their format.

In the following let's take a look how the outbox pattern can be implemented with Debezium.
This image shows an overview of the overall design:

TODO: update image
image::local_view_materialization.png[Local View Materalization]

Let's begin by setting up an instance of Postgres as the database for the order service.
For this, we need a dedicated service account which will be used to run the database container
(that's required due to the way the Postgres image handles file permissions internally):

[source]
$ oc create sa debezium -n amq-streams
$ oc adm policy add-scc-to-user anyuid system:serviceaccount:amq-streams:debezium

Then we can set up a new instance of Postgres:

[source]
$ oc new-app --name=postgresql debezium/example-postgres:0.9 \
    -e POSTGRES_USER=postgresuser \
    -e POSTGRES_PASSWORD=postgrespw \
    -e POSTGRES_DB=orderdb

And make it use the previously set up service account:

[source]
$ oc patch dc/postgresql --type merge -p '{ "spec" : { "template" : { "spec" : { "serviceAccountName" : "debezium" } } } }'

Once the database pod is up and running, we can deploy an instance of an example order service provided for this lab.
It is a Java EE application running on WildFly.
OpenShift comes with s2i support for WildFly, too, so it's very easy to deploy the service:

[source,sh]
----
$ oc new-app --name=orderservice openshift/wildfly-160-centos7:latest~https://github.com/gunnarmorling/microservices-lab \
    --context-dir=outbox/order-service \
    -e AB_PROMETHEUS_OFF=true \
    -e JAVA_OPTIONS=-Djava.net.preferIPv4Stack=true \
    -e POSTGRESQL_DATABASE=orderdb \
    -e POSTGRESQL_USER=postgresuser \
    -e POSTGRESQL_PASSWORD=postgrespw \
    -e POSTGRESQL_DATASOURCE=OrderDS
----

This uses the WildFly 16 s2i builder image to produce a deployment for a project from the given git repository.
It also sets up a datasource named "OrderDS" which makes the previously set up Postgres database available to the application.

Now let's take a look at how the order service application creates the events in the outbox table.
The relevant code lives in the OrderService class, with the `addOrder()` method looking like so:

[source,java]
----
@Transactional
public PurchaseOrder addOrder(PurchaseOrder order) {
    order = entityManager.merge(order);

    event.fire(OrderCreatedEvent.of(order));
    event.fire(InvoiceCreatedEvent.of(order));

    return order;
}
----

This uses the JPA entity manager to persist a new purchase order;
at the same time it creates two events derived from this order:
one representing the creation of this purchase order and another one representing the creation of a corresponding invoice.
These events are processed synchronously as part of the same transaction.
They are handled by the EventSender class, which observes all event types derived from `ExportedEvent` and serializes them into the "outbox" table:

[source,java]
----
@ApplicationScoped
public class EventSender {

    @PersistenceContext
    private EntityManager entityManager;

    public void onExportedEvent(@Observes ExportedEvent event) {
        OutboxEvent outboxEvent = new OutboxEvent(
                event.getAggregateType(),
                event.getAggregateId(),
                event.getType(),
                event.getPayload(),
                event.getTimestamp()
        );

        entityManager.persist(outboxEvent);
    }
}
----

The order service provides a REST API for placing purchase orders.
So let's invoke this API to create an order and examine the corresponding database entries.

[source]
----
$ oc exec -c kafka -i production-ready-kafka-0 -- curl -X POST -s -w "\n" \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://orderservice:8080/rest/orders -d @- <<'EOF'
    {
        "customerId" : "123",
        "orderDate" : "2019-01-31T12:13:01",
        "lineItems" : [
            {
                "item" : "Debezium in Action",
                "quantity" : 2,
                "totalPrice" : 39.98
            },
            {
                "item" : "Debezium for Dummies",
                "quantity" : 1,
                "totalPrice" : 29.99
            }
        ]
    }
EOF
----

Once the order has been created, get a shell in the Postgres container:

[source,sh]
----
$ oc rsh $(oc get pods -o name -l app=postgresql)
----

Then use _psql_ to get all records from the `purchaseorder` and `outboxevent` tables:

[source,sh]
----
psql -U postgresuser orderdb -c 'select * from "purchaseorder"'
psql -U postgresuser orderdb -c 'select * from "outboxevent"'
----

The structure of the latter one is interesting in particular.
It has the following columns:

* `id`: unique id of each message; can be used by consumers to detect any duplicate events, e.g. when restarting to read messages after a failure.
Generated when creating a new event.
* `aggregatetype`: the type of the _aggregate root_ to which a given event is related;
the idea being, leaning on the same concept of domain-driven design,
that exported events should refer to an aggregate
(https://martinfowler.com/bliki/DDD_Aggregate.html["a cluster of domain objects that can be treated as a single unit"]),
where the aggregate root provides the sole entry point for accessing any of the entities within the aggregate.
This could for instance be "purchase order" or "customer".
+
This value will be used to route events to corresponding topics in Kafka,
so there'd be a topic for all events related to purchase orders,
one topic for all customer-related events etc.
Note that also events pertaining to a child entity contained within one such aggregate should use that same type.
So e.g. an event representing the cancelation of an individual order line
(which is part of the purchase order aggregate)
should also use the type of its aggregate root, "order",
ensuring that also this event will go into the "order" Kafka topic.
* `aggregateid`: the id of the aggregate root that is affected by a given event; this could for instance be the id of a purchase order or a customer id;
Similar to the aggregate type, events pertaining to a sub-entity contained within an aggregate should use the id of the containing aggregate root,
e.g. the purchase order id for an order line cancelation event.
This id will be used as the key for Kafka messages later on.
That way, all events pertaining to one aggregate root or any of its contained sub-entities will go into the same partition of that Kafka topic,
which ensures that consumers of that topic will consume all the events related to one and the same aggregate in the exact order as they were produced.
* `type`: the type of event, e.g. "Order Created" or "Order Line Canceled". Allows consumers to trigger suitable event handlers.
* `payload`: a JSON structure with the actual event contents, e.g. containing a purchase order, information about the purchaser, contained order lines, their price etc.

Now it's time to set up an instance of Debezium's Postgres connector for exporting the events from the outbox table to Apache Kafka:

[source,sh]
----
$ oc exec -c kafka -i production-ready-kafka-0 -- curl -s -X PUT -w "\n" \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://debezium-connect-api:8083/connectors/outbox-connector/config -d @- <<'EOF'

    {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "tasks.max": "1",
        "database.hostname": "postgresql",
        "database.port": "5432",
        "database.user": "postgresuser",
        "database.password": "postgrespw",
        "database.dbname" : "orderdb",
        "database.server.name": "dbserver1",
        "schema.whitelist": "public",
        "table.whitelist" : "public.outboxevent",
        "tombstones.on.delete" : "false",
        "transforms" : "outbox",
        "transforms.outbox.type" : "io.debezium.transforms.outbox.EventRouter",
        "transforms.outbox.route.topic.replacement" : "${routedByValue}.events",
        "transforms.outbox.table.field.event.timestamp" : "timestamp"
    }
EOF
----

Besides the configuration parts we've seen before (database host name, credentials, table whitelist etc.),
there's a special SMT `EventRouter` applied.
This one comes with Debezium and serves the purpose for routing the events from an outbox table to specific topics.
It can be configured in many ways, but here we're using the default configuration mostly.
By default, the value from the `aggregatetype` column is used for topic routing.
By means of the `transforms.outbox.route.topic.replacement` option, the values from this column ("order", "customer" are used to derive topic names ("order.events", "customer.events").
The value from the `aggregateid` column is used as the message key,
ensuring that all events in one topic pertaining to the same entity (order, customer etc.) will go to the same partition of the corresponding Kafka topic.
If needed, the SMT could be configured to make us of other column for these purposes.

With the connector being deployed, we can take a look at the `order.events` topic:

[source]
oc exec -c zookeeper -it production-ready-zookeeper-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
   --bootstrap-server production-ready-kafka-bootstrap:9092 \
   --from-beginning \
   --property print.key=true \
   --topic order.events

Note how the event payload is a string-ified JSON, i.e. the event structure is opaque to the schema of the message in Kafka.

TODO: show event consumer

[source]
$ oc new-app --name=shipmentdb mariadb/server \
    -e MARIADB_USER=mariadbuser \
    -e MARIADB_PASSWORD=mariadbpw \
    -e MARIADB_DATABASE=shipmentdb \
    -e MARIADB_RANDOM_ROOT_PASSWORD=true

[source,sh]
----
$ oc logs $(oc get pods -o name -l app=cdc-consumer-app)
----

To wrap up this section, remove the resources we've created:

[source]
$ oc delete all -l app=orderservice
$ oc delete all -l app=postgresql

==== Bonus: Processing Data Change Events with Kafka Streams

If you still got some time left, let's explore how Debezium's data change events can be processed in a streaming query using the Kafka Streams API.
This API allows you to run operations on Kafka topics such as filtering, joining, aggregating etc. and can be a very powerful tool to gain real-time insight into your data as it changes.
Whenever new messages in the processed topics arrive, the KStreams pipeline will run and produce corresponding streaming query results,
which then for instance can be written into another topic.

The following example again is about the management of purchase orders,
which in this case belong to specific product categories such as "furniture", "toys" etc.
We're interested in the aggregated revenue per product category in sliding time windows.

We're going to deploy a producer application which creates new random purchase orders at a given rate.
Debezium is used to capture changes to the `orders` table and produce change events into a corresponding Kafka topic.
In a separate application, the KStreams pipeline for aggregating the revenue values is executed.

Let's begin by deploying a MySQL database which will hold the purchase orders:

[source]
$ oc new-app https://github.com/gunnarmorling/debezium-examples.git#kstreams-live-update-thorntail --strategy=docker --name=mysql --context-dir=kstreams-live-update/example-db \
    -e MYSQL_ROOT_PASSWORD=debezium \
    -e MYSQL_USER=mysqluser \
    -e MYSQL_PASSWORD=mysqlpw

Next we deploy the event producer application:

[source]
$ oc new-app --name=event-source debezium/msa-lab-s2i:latest~https://github.com/gunnarmorling/debezium-examples.git#kstreams-live-update-thorntail \
    --context-dir=kstreams-live-update/event-source \
    -e JAVA_MAIN_CLASS=io.debezium.examples.kstreams.liveupdate.eventsource.Main

It contains a simple Java main class that runs an https://github.com/debezium/debezium-examples/blob/master/kstreams-live-update/event-source/src/main/java/io/debezium/examples/kstreams/liveupdate/eventsource/EventSource.java[event source] which inserts random orders in a loop.

Use `oc get pods` to verify that both applications have been deployed and are running.

If you haven't done so yet, start an instance of Debezium's tooling container image in a separate shell session:

[source]
$ oc run tooling -it --image=debezium/tooling --restart=Never

Within the tooling pod, you can use `mycli` to see that new orders are created
(e.g. run `SELECT COUNT(1) FROM orders` repeatedly):

[source,sh]
mycli mysql://mysqluser@mysql:3306/inventory --password mysqlpw

Exit `mycli` (Ctrl + D).

Now let's deploy an instance of the Debezium connector for MySQL for capturing new purchase order and product category topics.
Still in the tooling pod, run this command:

[source,sh]
----
cat <<'EOF' > register-mysql-source.json

{
    "name": "mysql-source",
    "config": {
        "connector.class": "io.debezium.connector.mysql.MySqlConnector",
        "tasks.max": "1",
        "database.hostname": "mysql",
        "database.port": "3306",
        "database.user": "debezium",
        "database.password": "dbz",
        "database.server.id": "184055",
        "database.server.name": "dbserver1",
        "decimal.handling.mode" : "string",
        "table.whitelist": "inventory.orders,inventory.categories",
        "database.history.kafka.bootstrap.servers": "production-ready-kafka-bootstrap:9092",
        "database.history.kafka.topic": "schema-changes.inventory"
    }
}
EOF
cat register-mysql-source.json | http POST http://debezium-connect-api:8083/connectors/
----

With the connector being deployed, we can examine the contents of the Kafka topics for product categories and purchase orders:

[source,sh]
----
kafkacat -b production-ready-kafka-bootstrap -t dbserver1.inventory.categories -C -o beginning | jq ."payload"
----

[source,sh]
----
kafkacat -b production-ready-kafka-bootstrap -t dbserver1.inventory.orders -C -o end | jq ."payload"
----

The former doesn't show any activity, there are just the events from the initial snapshot of the categories table.
This is expected, as no new categories are added.
In contrast, the orders topic contains new messages for each newly produced record in the orders table.

Run the following in the other shell session (i.e. not within the tooling pod):

[source,sh]
----
$ oc new-app --name=aggregator debezium/msa-lab-s2i:latest~https://github.com/gunnarmorling/debezium-examples.git#kstreams-live-update-thorntail \
    --context-dir=kstreams-live-update/aggregator \
    -e AB_PROMETHEUS_OFF=true \
    -e KAFKA_BOOTSTRAP_SERVERS=production-ready-kafka-bootstrap:9092 \
    -e JAVA_OPTIONS=-Djava.net.preferIPv4Stack=true

$ oc patch dc/aggregator -p '[{"op": "add", "path": "/spec/template/spec/containers/0/ports/1", "value":{"containerPort":8080,"protocol":"TCP"}}]' --type=json

$ oc patch service aggregator -p '{ "spec" : { "ports" : [{ "name" : "8080-tcp", "port" : 8080, "protocol" : "TCP", "targetPort" : 8080 }] } } }'

$ oc expose svc aggregator
----

The most interesting part of this application is the https://github.com/debezium/debezium-examples/blob/master/kstreams-live-update/aggregator/src/main/java/io/debezium/examples/kstreams/liveupdate/aggregator/StreamsPipelineManager.java[StreamsPipelineManager] class,
which defines the Kafka Streams pipeline to run.
It looks like so:

[source,java]
----
KTable<Long, Category> category = builder.table("dbserver1.inventory.categories", Consumed.with(longKeySerde, categorySerde));

KStream<Windowed<String>, String> salesPerCategory = builder.stream(
        "dbserver1.inventory.orders",
        Consumed.with(longKeySerde, orderSerde)
        )

        // Join with categories on category id
        .selectKey((k, v) -> v.categoryId)
        .join(
                category,
                (value1, value2) -> {
                    value1.categoryName = value2.name;
                    return value1;
                },
                Joined.with(Serdes.Long(), orderSerde, null)
        )

        // Group by category name, windowed by 5 sec
        .selectKey((k, v) -> v.categoryName)
        .groupByKey(Serialized.with(Serdes.String(), orderSerde))
        .windowedBy(TimeWindows.of(Duration.ofSeconds(5).toMillis()))

        // Accumulate category sales per time window
        .aggregate(
                () -> 0L, /* initializer */
                (aggKey, newValue, aggValue) -> {
                    aggValue += newValue.salesPrice;
                    return aggValue;
                },
                Materialized.with(Serdes.String(), Serdes.Long())
        )
        .mapValues(v -> BigDecimal.valueOf(v)
                .divide(BigDecimal.valueOf(100), 2, RoundingMode.HALF_UP))
        .mapValues(v -> String.valueOf(v))

        // Push to WebSockets
        .toStream()
        .peek((k, v) -> {
            websocketsEndPoint.getSessions().forEach(s -> {
                try {
                    s.getBasicRemote().sendText("{ \"category\" : \"" + k.key() + "\", \"accumulated-sales\" : " + v + " }");
                }
                catch (IOException e) {
                    throw new RuntimeException(e);
                }
            });
});
----

It does the following things:

* Set up a `KTable` representing the current state of the categories topic
* Set up a `KStream` representing the orders topic; whenever there's a new message in that topic, the pipeline will be executed
* Join the orders stream with the categories table (this requires to choose the category id as the stream key, as joins are only possible if the key on both sides is the same);
The join result also contains the name of the category of the represented order
* Group the values by category name and build windows of the events with a time window size of 5 seconds
* Within each category and 5 second time window, sum up the value of all purchase orders
* Map the result value to a string and emit a JSON structure comprising the category name and aggregated revenue value via WebSockets

For the last step, the application also provides a web sockets endpoint.
The produced JSON structure will be pushed to all connected web sockets clients.
To see this in action, open the aggregator application in a web browser.
You can find its URL next to the "aggregator" application in the OpenShift web console or
by running:

[source]
$ oc get routes aggregator -o=jsonpath='{.spec.host}{"\n"}'

You should see a simple chart which is updated when ever new revenue values are sent to the browser.

==== Environment cleanup

tbd.

=== Summary

In this part of the lab you've learned about the concept of change data capture and how to implement it using Debezium and Kafka (Connect).
You've set up the Debezium connector for MySQL to ingest changes of an existing Java EE application,
without requiring any code changes to that application.
Then you've explored different ways for consuming the change events:
using Kafka Connect and the JDBC sink adaptor to simply stream the data into a PostgreSQL database
and using Thorntail and CDI to consume change events programmatically and relay them to a web browser using WebSockets.

To learn more about Debezium, refer to its homepage http://debezium.io[https://debezium.io/],
where you can find an extensive tutorial, documentation and more.
